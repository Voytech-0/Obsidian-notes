# 3. ML methods
22:11:2509:41
Status:  #MachineLearning
Tags: 

---
# Regression models
Output of regression is continuous

## Linear regression
$$f(x) = \sum_i {x_i w_i }\ +b = x*w +b$$
Here the parameter/weight vector $w$ has the same length as the input features $x$, and the bias $b$ is a scalar.

### Matrix formulation 
The previous equation can be written in a matrix form. $$X = \begin{bmatrix} x & 1\end{bmatrix}$$ $$W = \begin{bmatrix} \textbf {w} \\ b\end{bmatrix}$$
$$f(x) = XW = \begin{bmatrix} \textbf  x & 1\end{bmatrix} \begin{bmatrix} \textbf  w \\ b\end{bmatrix} = x*w +b$$
### Closed Form Solution
Linear regression models are trained using mean squared error loss (with $n$ data points):
$$L(y, \hat{y})=n^{-1} \sum_i\left(W X_i-y_i\right)^2$$
![[Pasted image 20221125100439.png|800]]
![[Pasted image 20221125100556.png]]
It only works if the rows of X are *linearly independent*. It only makes sense to do this if the matrix inverse  
$(X^T X)^{−1}$ is tractable, and if the matrix multiplication $X^T X$ is computationally sane.

#### Model Assumptions
- Residuals:
	- $f(x_i) = WX_i + \epsilon_i = y_i$
- Data must not have dependencies.
	- Variables must not be correlated
- Independence of residuals
	-  The residuals $\epsilon_i$ are assumed to be independent and not related to each other or to the input variables. This means that the errors ado not depend on the input.
- Linearity
	- In this model, the input features $x_i$ are treated as fixed values, and the model is only linear with respect of its parameter. Input features can be transformed to produce other features and the model stays being linear, since the training data is treated as a constant, and optimisation only happens in the parameters W.
- Constant variance
	- The variance of the residuals does not depend in the inputs or the outputs of the model. This is called [[homoskedastic]]ity. The opposite ([[heteroskedastic]]ity) is when the variance of the output or the errors can vary with the input or output of the model, for example, if larger outputs have larger variance then smaller outputs.

---
### Solution with Gradient Descent
In case that the analytic solution is intractable, gradient descent can be used as substitute. For this we need to compute the gradient of the loss with respect to parameters:
![[Pasted image 20221125101943.png|400]]
This has a well known closed form:
![[Pasted image 20221125102059.png|400]]
![[Pasted image 20221125102149.png|600]]

### Multi-variable Linear Regression
What if the labels y are not scalars, but vectors of dimension m? We can still perform linear regression, but now the model outputs a vector instead of a scalar. This can be seen as performing m individual linear regression problems: $f(x) = \begin{bmatrix} x_j * w_j + b_j \end{bmatrix}_j = WX + b$ Where now b is a m × 1 vector instead of a scalar, and W is a d × m matrix instead of a vector.

### Interpretability of weights
For linear models like LR, the weights/parameters can sometimes be interpreted if:
-   Input features are normalized/scaled to be in the exact same range.
-   For the case of multi-variable LR, then the output features should also be normalized/scaled to be at the same range.
### Polynomial regression
$$f(x) = \sum_{i=0}^p{w_ix^i} = w_o + w_1x + w_2x^2 + ... + w_px^p$$
Which is a linear model on the features $\begin{bmatrix}1, x, x^2, …, x^p\end{bmatrix}$
This is called polynomial regression, and it is a way to make regression non-linear, by modifying the input features. You choose a value of p (using cross validation), transform each feature xi into p polynomial values, and train a linear regression model on the new features.

This method increases the feature space dimensionality by a factor of p

### Robustness of Linear Regression
Linear Regression is overall not robust to outliers. There are many alternatives.
-   There are many robust linear regression methods, generally making assumptions about the output y, for example that it follows a student’s t-distribution or other heavy tailed distribution.
-   The simple LR algorithm assumes that the output is Gaussian distributed, making it not robust to outliers.
-   RANSAC (Random Sampling Consensus) can be used to fit multiple LR models and detect which ones are outliers.
-   Exploratory data analysis can be used to identify and remove outliers.

## Logistic regression
### Binary classifier
#### Perceptron
A basic model of binary classifier. $$f(x) = sgn(\sum_iw_ix_i + b) = sgn(w*x+b)$$
The function $sgn$ is the sign function defined by:
$$\operatorname{sgn}(x)= \begin{cases}-1 & \text { if } x<0 \\ 1 & \text { if } x \geq 0\end{cases}$$
### Decision Boundary
- The equation $w*x+b = 0$ defines a hyper-plane in the feature space x.
- This hyper-plane is called the decision boundary. For a linear classifier is a line in 2D, a plane in 3D, and a hyper-plane in more than three dimensions. For a non-linear classifier, the decision boundary is a curve in feature space.
- Each side of the boundary defines a region for each class.
- Usually the points close to the boundary are the most difficulty to classify, as due to noise and position of the boundary, they could fall in either side of the boundary.
# References